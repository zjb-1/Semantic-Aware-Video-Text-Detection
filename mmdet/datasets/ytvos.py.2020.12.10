import numpy as np
import os.path as osp
import random
import mmcv
import cv2
cv2.setNumThreads(0)
cv2.ocl.setUseOpenCL(False)
from .custom import CustomDataset
from .registry import DATASETS
from pycocotools.ytvos import YTVOS
from mmcv.parallel import DataContainer as DC
from .utils import to_tensor, random_scale
from .extra_aug import ExtraAugmentation
from .transforms import (ImageTransform, BboxTransform, MaskTransform,
                         Numpy2Tensor)


@DATASETS.register_module
class YTVOSDataset(CustomDataset):
    CLASSES = ('person','giant_panda','lizard','parrot','skateboard','sedan',
        'ape','dog','snake','monkey','hand','rabbit','duck','cat','cow','fish',
        'train','horse','turtle','bear','motorbike','giraffe','leopard',
        'fox','deer','owl','surfboard','airplane','truck','zebra','tiger',
        'elephant','snowboard','boat','shark','mouse','frog','eagle','earless_seal',
        'tennis_racket')
    def __init__(self,
                 ann_file,
                 img_prefix,
                 img_scale,
                 img_norm_cfg,
                 proposal_file=None,
                 size_divisor=None,
                 num_max_proposals=1000,
                 flip_ratio=0,
                 with_mask=True,
                 with_crowd=True,
                 with_label=True,
                 with_track=False,
                 extra_aug=None,
                 aug_ref_bbox_param=None,
                 resize_keep_ratio=True,
                 test_mode=False):

        # prefix of images path
        self.img_prefix = img_prefix

        # load annotations
        self.vid_infos = self.load_annotations(ann_file)
        img_ids = []
        for idx, vid_info in enumerate(self.vid_infos):
            for frame_id in range(len(vid_info['filenames'])):
                img_ids.append((idx, frame_id))
        self.img_ids = img_ids
        if proposal_file is not None:
            self.proposals = self.load_proposals(proposal_file)
        else:
            self.proposals = None

        # filter images with no annotation during training
        if not test_mode:
            valid_inds = [i for i, (v, f) in enumerate(self.img_ids)
                if len(self.get_ann_info(v, f)['bboxes'])]
            self.img_ids = [self.img_ids[i] for i in valid_inds]

        # (long_edge, short_edge) or [(long1, short1), (long2, short2), ...]
        self.img_scales = img_scale if isinstance(img_scale,
                                                  list) else [img_scale]
        assert mmcv.is_list_of(self.img_scales, tuple)

        # normlization configs
        self.img_norm_cfg = img_norm_cfg
        # flip ratio
        self.flip_ratio = flip_ratio
        assert flip_ratio >= 0 and flip_ratio <= 1
        # padding border to ensure the image size can be diviced by size divisor
        self.size_divisor = size_divisor

        # with mask
        self.with_mask = with_mask
        self.with_crowd = with_crowd
        self.with_label = with_label
        self.with_track = with_track

        # params for augmenting bbox in the reference frame
        self.aug_ref_bbox_param = aug_ref_bbox_param
        self.test_mode = test_mode

        # set group flags for the sampler
        if not self.test_mode:
            self._set_group_flag()

        # transforms
        self.img_transform = ImageTransform(
            size_divisor=self.size_divisor, **self.img_norm_cfg)
        self.bbox_transform = BboxTransform()
        self.mask_transform = MaskTransform()
        self.numpy2tensor = Numpy2Tensor()

        # if use extra augmentation
        if extra_aug is not None:
            self.extra_aug = ExtraAugmentation(**extra_aug)
        else:
           self.extra_aug = None
        # image rescale if keep ratios
        self.resize_keep_ratio = resize_keep_ratio


    def __len__(self):
        return len(self.img_ids)

    def _set_group_flag(self):
        """Set flag according to image aspect ratio.
        Images with aspect ratio greater than 1 will be set as group 1,
        otherwise group 0
        """
        self.flag = np.zeros(len(self), dtype=np.uint8)
        for i in range(len(self)):
            vid_id, _ = self.img_ids[i]
            vid_info = self.vid_infos[vid_id]
            if vid_info['width'] / vid_info['height'] > 1:
                self.flag[i] = 1

    def load_annotations(self, ann_file):
        self.ytvos = YTVOS(ann_file)
        self.cat_ids = self.ytvos.getCatIds()
        self.cat2label = {
            cat_id: i + 1
            for i, cat_id in enumerate(self.cat_ids)
        }
        self.vid_ids = self.ytvos.getVidIds()
        vid_infos = []
        for i in self.vid_ids:
            info = self.ytvos.loadVids([i])[0]
            info['filenames'] = info['file_names']
            vid_infos.append(info)
        return vid_infos

    def get_ann_info(self, idx, frame_id):
        vid_id = self.vid_infos[idx]['id']
        ann_ids = self.ytvos.getAnnIds(vidIds=[vid_id])
        ann_info = self.ytvos.loadAnns(ann_ids)
        return self._parse_ann_info(ann_info, frame_id)

    def __getitem__(self, idx):
        if self.test_mode:
            return self.prepare_test_img(self.img_ids[idx])
        data = self.prepare_train_img(self.img_ids[idx])
        return data

    def sample_ref(self, idx):
        # sample another frame in the same sequence as reference
        vid, frame_id = idx
        vid_info = self.vid_infos[vid]
        sample_range = range(len(vid_info['filenames']))
        valid_samples = []
        for i in sample_range:
            # check if the frame id is valid
            ref_id = (vid, i)
            #if i != frame_id and ref_id in self.img_ids:
            # frame id is close
            #if i != frame_id and ref_id in self.img_ids and abs(i - frame_id) <= 5:
            # frame id is close or far
            #if i != frame_id and ref_id in self.img_ids and (abs(i - frame_id) <= 5 or (abs(i - frame_id) <= 150 and abs(i - frame_id) >= 130)):
            # frame id is far
            if i != frame_id and ref_id in self.img_ids and abs(i - frame_id) <= 5:
                valid_samples.append(ref_id)
        assert len(valid_samples) > 0
        return random.choice(valid_samples)

    def bbox_aug(self, bbox, img_size):
        assert self.aug_ref_bbox_param is not None
        center_off = self.aug_ref_bbox_param[0]
        size_perturb = self.aug_ref_bbox_param[1]

        n_bb = bbox.shape[0]
        # bbox center offset
        center_offs = (2*np.random.rand(n_bb, 2) - 1) * center_off
        # bbox resize ratios
        resize_ratios = (2*np.random.rand(n_bb, 2) - 1) * size_perturb + 1
        # bbox: x1, y1, x2, y2
        centers = (bbox[:,:2]+ bbox[:,2:])/2.
        sizes = bbox[:,2:] - bbox[:,:2]
        new_centers = centers + center_offs * sizes
        new_sizes = sizes * resize_ratios
        new_x1y1 = new_centers - new_sizes/2.
        new_x2y2 = new_centers + new_sizes/2.
        c_min = [0,0]
        c_max = [img_size[1], img_size[0]]
        new_x1y1 = np.clip(new_x1y1, c_min, c_max)
        new_x2y2 = np.clip(new_x2y2, c_min, c_max)
        bbox = np.hstack((new_x1y1,new_x2y2)).astype(np.float32)
        return bbox

    def prepare_train_img(self, idx):
        # prepare a pair of image in a sequence
        vid, frame_id = idx
        vid_info = self.vid_infos[vid]

        # load image
        img = mmcv.imread(osp.join(self.img_prefix, vid_info['filenames'][frame_id]))
        _, ref_frame_id = self.sample_ref(idx)
        #print('frame_id is ' + str(frame_id) + ' ref id is ' + str(ref_frame_id))
        ref_img = mmcv.imread(osp.join(self.img_prefix, vid_info['filenames'][ref_frame_id]))

        # load proposals
        if self.proposals is not None:
            results['proposals'] = self.proposals[idx]
        ann = self.get_ann_info(vid, frame_id)
        ref_ann = self.get_ann_info(vid, ref_frame_id)

        gt_bboxes = ann['bboxes']
        gt_labels = ann['labels']

        ref_bboxes = ref_ann['bboxes']
        ref_ids = ref_ann['obj_ids']
        gt_ids = ann['obj_ids']

        gt_pids = [ref_ids.index(i)+1 if i in ref_ids else 0 for i in gt_ids]
        vis = False
        if (vis):
            gt_masks = ann['masks']
            print(frame_id)
            print(gt_ids)
            print(ref_ids)
            print(gt_pids)           
            #print(gt_masks.shape)
            # write gt_bbox
            im_name_box = str(frame_id) + '_box.jpg'
            ref_name_box = str(frame_id) + '_refbox.jpg'
            print(im_name_box)
            print(ref_name_box)
            print(gt_bboxes.shape)
            print(ref_bboxes.shape)
            print(vid_info['filenames'][frame_id])
            point_color = (0, 0, 255) # BGR
            thickness = 1
            lineType = 8
            img_vis = img
            ref_img_vis = ref_img
            for box_id in range(gt_bboxes.shape[0]):
                lefttop = (gt_bboxes[box_id][0], gt_bboxes[box_id][1])
                rightbot = (gt_bboxes[box_id][2], gt_bboxes[box_id][3])
                cv2.rectangle(img_vis, lefttop, rightbot, point_color, thickness, lineType)
            #cv2.imwrite(im_name_box, img_vis)
            for box_id in range(ref_bboxes.shape[0]):
                lefttop = (ref_bboxes[box_id][0], ref_bboxes[box_id][1])
                rightbot = (ref_bboxes[box_id][2], ref_bboxes[box_id][3])
                cv2.rectangle(ref_img_vis, lefttop, rightbot, point_color, thickness, lineType)
            #cv2.imwrite(ref_name_box, ref_img_vis)
            im_name_mask = osp.join(vid_info['filenames'][frame_id], '_mask.jpg')
            #cv2.imwrite(im_name_mask, gt_masks)
            
            

        if self.with_crowd:
            gt_bboxes_ignore = ann['bboxes_ignore']
        # skip the image if there is no valid gt bbox
        if len(gt_bboxes) == 0:
            return None

        # extra augmentation
        if self.extra_aug is not None:
            img, gt_bboxes, gt_labels = self.extra_aug(img, gt_bboxes, gt_labels)

        # apply transforms
        flip = True if np.random.rand() < self.flip_ratio else False
        img_scale = random_scale(self.img_scales, 'value') # sample a scale
        img, img_shape, pad_shape, scale_factor = self.img_transform(img,
                                                                     img_scale,
                                                                     flip,
                                                                     keep_ratio=self.resize_keep_ratio)
        img = img.copy()
        ref_img, ref_img_shape, ref_img_shape, ref_scale_factor = self.img_transform(
                                                                    ref_img,
                                                                    img_scale,
                                                                    flip,
                                                                    keep_ratio=self.resize_keep_ratio)
        ref_img = ref_img.copy()
        gt_bboxes = self.bbox_transform(gt_bboxes, img_shape, scale_factor, flip)
        ref_bboxes = self.bbox_transform(ref_bboxes, ref_img_shape, ref_scale_factor, flip)

        if self.aug_ref_bbox_param is not None:
            ref_bboxes = self.bbox_aug(ref_bboxes, ref_img_shape)

        if self.with_crowd:
            gt_bboxes_ignore = self.bbox_transform(gt_bboxes_ignore, img_shape,
                                                   scale_factor, flip)

        if self.with_mask:
            gt_masks = self.mask_transform(ann['masks'], pad_shape,
                                           scale_factor, flip)

        ori_shape = (vid_info['height'], vid_info['width'], 3)
        img_meta = dict(
            ori_shape=ori_shape,
            img_shape=img_shape,
            pad_shape=pad_shape,
            scale_factor=scale_factor,
            flip=flip)

        data = dict(
            img=DC(to_tensor(img), stack=True),
            ref_img=DC(to_tensor(ref_img), stack=True),
            img_meta=DC(img_meta, cpu_only=True),
            gt_bboxes=DC(to_tensor(gt_bboxes)),
            ref_bboxes = DC(to_tensor(ref_bboxes))
        )
        if self.proposals is not None:
            data['proposals'] = DC(to_tensor(proposals))
        if self.with_label:
            data['gt_labels'] = DC(to_tensor(gt_labels))
        if self.with_track:
            data['gt_pids'] = DC(to_tensor(gt_pids))
        if self.with_crowd:
            data['gt_bboxes_ignore'] = DC(to_tensor(gt_bboxes_ignore))
        if self.with_mask:
            data['gt_masks'] = DC(gt_masks, cpu_only=True)
        return data

    def prepare_single(self, img, frame_id, vid, vid_info, scale, flip, proposal=None):
        _img, img_shape, pad_shape, scale_factor = self.img_transform(
            img, scale, flip, keep_ratio=self.resize_keep_ratio)
        _img = to_tensor(_img)
        _img_meta = dict(
            ori_shape=(vid_info['height'], vid_info['width'], 3),
            img_shape=img_shape,
            pad_shape=pad_shape,
            is_first=(frame_id == 0),
            video_id=vid,
            frame_id =frame_id,
            scale_factor=scale_factor,
            flip=flip)
        if proposal is not None:
            if proposal.shape[1] == 5:
                score = proposal[:, 4, None]
                proposal = proposal[:, :4]
            else:
                score = None
            _proposal = self.bbox_transform(proposal, img_shape,
                                            scale_factor, flip)
            _proposal = np.hstack(
                [_proposal, score]) if score is not None else _proposal
            _proposal = to_tensor(_proposal)
        else:
            _proposal = None
        return _img, _img_meta, _proposal


    def prepare_test_img(self, idx):
        """ prepare an image for testing """
        vid, frame_id = idx
        vid_info = self.vid_infos[vid]
        img = mmcv.imread(osp.join(self.img_prefix, vid_info['filenames'][frame_id]))
        print(vid_info['filenames'][frame_id])
        proposal = None

        imgs = []
        img_metas = []
        proposals = []
        for scale in self.img_scales:
            _img, _img_meta, _proposal = self.prepare_single(
                img, frame_id, vid, vid_info, scale, False, proposal)
            imgs.append(_img)
            img_metas.append(DC(_img_meta, cpu_only=True))
            proposals.append(_proposal)
            if self.flip_ratio > 0:
                _img, _img_meta, _proposal = self.prepare_single(
                    img, frame_id, vid, vid_info, scale, True, proposal)
                imgs.append(_img)
                img_metas.append(DC(_img_meta, cpu_only=True))
                proposals.append(_proposal)
        data = dict(img=imgs, img_meta=img_metas)
        return data


    def _parse_ann_info(self, ann_info, frame_id, with_mask=True):
        gt_bboxes = []
        gt_labels = []
        gt_ids = []
        gt_bboxes_ignore = []

        if with_mask:
            gt_masks = []
            gt_masks_polys = []
            gt_poly_lens = []
        for i, ann in enumerate(ann_info):

            bbox = ann['bboxes'][frame_id]
            area = ann['areas'][frame_id]
            segm = ann['segmentations'][frame_id]
            if bbox is None:
                continue
            if bbox == 'None':
                continue
            x1, y1, w, h = bbox
            if area <= 0 or w < 1 or h < 1:
                continue
            bbox = [x1, y1, x1 + w - 1, y1 + h - 1]

            if ann['iscrowd']:
                gt_bboxes_ignore.append(bbox)
            else:
                gt_bboxes.append(bbox)
                gt_ids.append(ann['id'])
                gt_labels.append(self.cat2label[ann['category_id']])

            if with_mask:
                gt_masks.append(self.ytvos.annToMask(ann, frame_id))
                mask_polys = []#[p for p in segm if len(p) > 6]
                poly_lens = []#[len(p) for p in mask_polys]
                gt_masks_polys.append(mask_polys)
                gt_poly_lens.append(poly_lens)

        if gt_bboxes:
            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)
            gt_labels = np.array(gt_labels, dtype=np.int64)
        else:
            gt_bboxes = np.zeros((0, 4), dtype=np.float32)
            gt_labels = np.array([], dtype=np.int64)

        if gt_bboxes_ignore:
           gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)
        else:
           gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)


        ann = dict(
            bboxes=gt_bboxes,
            labels=gt_labels,
			obj_ids=gt_ids,
            bboxes_ignore=gt_bboxes_ignore)
        if with_mask:
            ann['masks'] = gt_masks
            ann['mask_polys'] = gt_masks_polys
            ann['poly_lens'] = gt_poly_lens
        return ann




